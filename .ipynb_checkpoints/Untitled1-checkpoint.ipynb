{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7e2a5f-e3a2-4ccf-a45f-7004ca8a6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/databrickslabs/dbldatagen\n",
      "  Cloning https://github.com/databrickslabs/dbldatagen to /tmp/pip-req-build-qm5f8emk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/databrickslabs/dbldatagen /tmp/pip-req-build-qm5f8emk\n",
      "  Resolved https://github.com/databrickslabs/dbldatagen to commit 14a1bece08f8cebfe9ad050301f32f8afa92f875\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: dbldatagen\n",
      "  Building wheel for dbldatagen (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dbldatagen: filename=dbldatagen-0.2.0rc1-py3-none-any.whl size=68973 sha256=784721f0f7a297aec904e31b0610455d604ecc5ae198a029d0adf20783703595\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u5_58xs2/wheels/fa/2f/29/fd337c2f0a1da95c1069d7f4898c3ff5a0ea697cb5d31f319c\n",
      "Successfully built dbldatagen\n",
      "Installing collected packages: dbldatagen\n",
      "Successfully installed dbldatagen-0.2.0rc1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/databrickslabs/dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7196c537-1f72-49f9-9a79-86692887960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg\n",
    "\n",
    "shuffle_partitions_requested = 12 * 4\n",
    "partitions_requested = 96\n",
    "device_population = 100000\n",
    "data_rows = 1000 * 100000\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "country_codes = ['CN', 'US', 'FR', 'CA', 'IN', 'JM', 'IE', 'PK', 'GB', 'IL', 'AU', 'SG', 'ES',\n",
    "                 'GE', 'MX', 'ET', 'SA',\n",
    "                 'LB', 'NL']\n",
    "country_weights = [1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, 126, 109, 58, 8,\n",
    "                   17]\n",
    "\n",
    "manufacturers = ['Delta corp', 'Xyzzy Inc.', 'Lakehouse Ltd', 'Acme Corp', 'Embanks Devices']\n",
    "\n",
    "lines = ['delta', 'xyzzy', 'lakehouse', 'gadget', 'droid']\n",
    "\n",
    "testDataSpec = (dg.DataGenerator(spark, name=\"device_data_set\", rows=data_rows,\n",
    "                                 partitions=partitions_requested, randomSeedMethod='hash_fieldname')\n",
    "                .withIdOutput()\n",
    "                # we'll use hash of the base field to generate the ids to \n",
    "                # avoid a simple incrementing sequence\n",
    "                .withColumn(\"internal_device_id\", LongType(), minValue=0x1000000000000,\n",
    "                            uniqueValues=device_population, omit=True, baseColumnType=\"hash\")\n",
    "\n",
    "                # note for format strings, we must use \"%lx\" not \"%x\" as the \n",
    "                # underlying value is a long\n",
    "                .withColumn(\"device_id\", StringType(), format=\"0x%013x\",\n",
    "                            baseColumn=\"internal_device_id\")\n",
    "\n",
    "                # the device / user attributes will be the same for the same device id \n",
    "                # so lets use the internal device id as the base column for these attribute\n",
    "                .withColumn(\"country\", StringType(), values=country_codes,\n",
    "                            weights=country_weights,\n",
    "                            baseColumn=\"internal_device_id\")\n",
    "                .withColumn(\"manufacturer\", StringType(), values=manufacturers,\n",
    "                            baseColumn=\"internal_device_id\")\n",
    "\n",
    "                # use omit = True if you don't want a column to appear in the final output \n",
    "                # but just want to use it as part of generation of another column\n",
    "                .withColumn(\"line\", StringType(), values=lines, baseColumn=\"manufacturer\",\n",
    "                            baseColumnType=\"hash\", omit=True)\n",
    "                .withColumn(\"model_ser\", IntegerType(), minValue=1, maxValue=11,\n",
    "                            baseColumn=\"device_id\",\n",
    "                            baseColumnType=\"hash\", omit=True)\n",
    "\n",
    "                .withColumn(\"model_line\", StringType(), expr=\"concat(line)\",\n",
    "                            baseColumn=[\"line\", \"model_ser\"])\n",
    "                .withColumn(\"event_type\", StringType(),\n",
    "                            values=[\"activation\", \"deactivation\", \"plan change\",\n",
    "                                    \"telecoms activity\", \"internet activity\", \"device error\"],\n",
    "                            random=True)\n",
    "                .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", end=\"2020-12-31 23:59:00\", interval=\"1 minute\", random=True)\n",
    "\n",
    "                )\n",
    "\n",
    "dfTestData = testDataSpec.build()\n",
    "\n",
    "# dfTestData.write.format(\"csv\").mode(\"overwrite\").save(\n",
    "#     \"gs://spark-poc-ca/dbldatagen_examples/a_billion_row_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f24a98-0515-4172-937a-25dfc1ba968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/databrickslabs/dbldatagen\n",
      "  Cloning https://github.com/databrickslabs/dbldatagen to /tmp/pip-req-build-ko4bwkyi\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/databrickslabs/dbldatagen /tmp/pip-req-build-ko4bwkyi\n",
      "  Resolved https://github.com/databrickslabs/dbldatagen to commit 14a1bece08f8cebfe9ad050301f32f8afa92f875\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: dbldatagen\n",
      "  Building wheel for dbldatagen (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dbldatagen: filename=dbldatagen-0.2.0rc1-py3-none-any.whl size=68973 sha256=e884a1d117be0e126707688e7231c12c5e1e4007a1d85dc91d96ecb79e3a9ed1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mla35a7v/wheels/fa/2f/29/fd337c2f0a1da95c1069d7f4898c3ff5a0ea697cb5d31f319c\n",
      "Successfully built dbldatagen\n",
      "Installing collected packages: dbldatagen\n",
      "Successfully installed dbldatagen-0.2.0rc1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/databrickslabs/dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8124d2a-bd13-49f5-90c4-6872c23ebff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+---------------+----------+-----------------+-------------------+\n",
      "| id|      device_id|country|   manufacturer|model_line|       event_type|           event_ts|\n",
      "+---+---------------+-------+---------------+----------+-----------------+-------------------+\n",
      "|  0|0x100000001281d|     CN|     Xyzzy Inc.| lakehouse|     deactivation|2020-12-30 21:02:00|\n",
      "|  1|0x1000000013b1d|     US|     Delta corp|    gadget|internet activity|2020-07-22 16:02:00|\n",
      "|  2|0x1000000011c18|     CN|Embanks Devices| lakehouse|     deactivation|2020-07-07 10:33:00|\n",
      "|  3|0x10000000050e3|     IN|      Acme Corp|     droid|internet activity|2020-12-07 20:24:00|\n",
      "|  4|0x1000000003674|     CN|     Xyzzy Inc.| lakehouse|       activation|2020-05-28 14:10:00|\n",
      "|  5|0x100000001492c|     CN|Embanks Devices| lakehouse|internet activity|2020-05-31 02:17:00|\n",
      "|  6|0x100000000b185|     CN|     Xyzzy Inc.| lakehouse|telecoms activity|2020-02-07 21:24:00|\n",
      "|  7|0x10000000041ab|     IN|  Lakehouse Ltd|     xyzzy|      plan change|2020-03-12 11:03:00|\n",
      "|  8|0x1000000003e2c|     IN|  Lakehouse Ltd|     xyzzy|telecoms activity|2020-02-28 02:41:00|\n",
      "|  9|0x100000000aa81|     IN|     Delta corp|    gadget|      plan change|2020-09-10 21:12:00|\n",
      "| 10|0x100000000d97d|     AU|      Acme Corp|     droid|     device error|2020-06-29 23:45:00|\n",
      "| 11|0x10000000050f1|     IN|  Lakehouse Ltd|     xyzzy|internet activity|2020-04-25 01:32:00|\n",
      "| 12|0x10000000058b8|     CN|      Acme Corp|     droid|internet activity|2020-12-16 19:05:00|\n",
      "| 13|0x10000000123f7|     PK|Embanks Devices| lakehouse|       activation|2020-04-27 11:48:00|\n",
      "| 14|0x1000000008f92|     ET|     Delta corp|    gadget|telecoms activity|2020-06-23 19:46:00|\n",
      "| 15|0x100000000417c|     IN|     Delta corp|    gadget|     device error|2020-06-26 21:55:00|\n",
      "| 16|0x100000000cefe|     CN|     Xyzzy Inc.| lakehouse|internet activity|2020-06-27 23:35:00|\n",
      "| 17|0x1000000010aa2|     CN|Embanks Devices| lakehouse|telecoms activity|2020-01-06 15:17:00|\n",
      "| 18|0x10000000023be|     IN|     Xyzzy Inc.| lakehouse|telecoms activity|2020-09-08 17:15:00|\n",
      "| 19|0x1000000001134|     IN|     Delta corp|    gadget|internet activity|2020-05-25 15:30:00|\n",
      "+---+---------------+-------+---------------+----------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTestData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cf8e77-034c-44ea-9e48-e05c1ce12f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfTestData.write.format(\"csv\").mode(\"overwrite\").save(\n",
    "    \"gs://spark-poc-ca/dbldatagen_examples/a_billion_row_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "417cb22a-d938-458c-a4b4-60983e0430a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfTestData.coalesce(1).write.csv(\"gs://spark-poc-ca/test/a_billion_row_table3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d6459e-8457-44d7-acd3-70e9b936ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfTestData.repartition(1).write.csv(\"gs://spark-poc-ca/test/a_billion_row_table4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d37e43-4053-4345-92f1-68acda7e32ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
